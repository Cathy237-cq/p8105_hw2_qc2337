---
title: "p8105_hw2_qc2337"
author: "cathy"
date: "2024-10-02"
output: github_document
---
Load all the package needed

```{r setup, echo = FALSE, message=FALSE}
library(tidyverse)
library(readxl)
```

## Problem 1

 Read and clean data

```{r}
 nyc_transit_df =
   read_csv("NYC_Transit_Subway_Entrance_And_Exit_Data.csv")|> 
   janitor::clean_names() |> 
   select(line, station_name, station_latitude, station_longitude, 
          route1:route11, entrance_type, entry, vending, ada) |> 
   mutate(
      entry = case_match(
       entry,
        "YES" ~ TRUE,
        "NO" ~ FALSE
    )
  )
```

### Short description for the dataset
In this 'nyc_transit_df' dataset, there are 19 variables including line, station_name, station_latitude, station_longitude, route1 to route 11, entrance_type, entry, vending, and ada.

Steps for data cleaning include: 
 1. Make all variables lowcase.
 2. Select the variables I need.
 3. Using 'mutate' to convert the entry variable from a character to a logical variable.

I got the dataset with 1,868 × 19. These data are not tidy because there are 11 columns to contain 'route', it should be included in 1 column.

### Answer the following questions using these data:

1. How many distinct stations are there? 

```{r}
 nycstation_df = 
   nyc_transit_df |> 
   distinct(line, station_name)

 nycstation_df
```

Create a new variable 'nycstation_df', using 'distinct' we got 465 distinct stations.

2. How many stations are ADA compliant?

```{r}
 ada_station_df =
   nyc_transit_df |> 
     select(ada, line, station_name) |>
     filter(ada) |> 
     distinct(line, station_name)

 ada_station_df 
```

 84 stations are ADA compliant.

3. What proportion of station entrances / exits without vending allow entrance?

```{r}
 no_vending_entry = 
   nyc_transit_df |> 
      filter(vending == "NO", entry == TRUE)

 no_vending_station =
   nyc_transit_df |> 
   filter(vending == "NO")
 
 
proportion = nrow(no_vending_entry) / nrow(no_vending_station)

proportion

```

The proportion of station entrances / exits without vending allow entrance is 0.3770492. 


### Reformat data

Make route number and route name distinct variables
and answer 2 questions

```{r}
 nyc_routes =
   nyc_transit_df |> 
   mutate(across(route1:route11, as.character)) |> 
   pivot_longer(
    cols = route1:route11,
    names_to = "route_number",
    values_to = "route_value",
    names_prefix = "route"
) |> 
   filter(route_value == "A", ada == TRUE) |> 
   distinct(line, station_name)
  
 nyc_routes
```

60 distinct stations serve the A train.
Of the stations that serve the A train, 17 are ADA compliant.


## Problem 2

Read and clean the Mr. Trash Wheel sheet
 
```{r}
mrtrash_wheel_df = 
  read_excel(
    "202409 Trash Wheel Collection Data.xlsx", 
    skip = 1, sheet = "Mr. Trash Wheel") |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(sports_balls = as.integer(round(sports_balls))) |> 
  select(-"x15", -"x16")
```


Read and clean the Professor Trash Wheel sheet

```{r}
proftrash_wheel_df = 
  read_excel(
    "202409 Trash Wheel Collection Data.xlsx", 
    skip = 1, sheet = "Professor Trash Wheel") |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(year = as.character(year))
```


Read and clean the Gwynnda Trash Wheel sheet

```{r}
gwytrash_wheel_df = 
  read_excel(
    "202409 Trash Wheel Collection Data.xlsx", 
    skip = 1, sheet = "Gwynnda Trash Wheel") |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(year = as.character(year))
```


Add an additional variable, name 'trash_wheel_name',
to keep track of different 'Trash Wheel'

```{r}
 mrtrash_wheel_df =
   mrtrash_wheel_df |> 
   mutate(trash_wheel_name = "Mr. Trash Wheel")

 proftrash_wheel_df =
   proftrash_wheel_df |> 
   mutate(trash_wheel_name = "Professor Trash Wheel")

 gwytrash_wheel_df = 
   gwytrash_wheel_df |> 
   mutate(trash_wheel_name = "Gwynnda Trash Wheel")
```


Combine the 3 excel sheets

```{r}
 combine_df =
   bind_rows(mrtrash_wheel_df, proftrash_wheel_df,    
             gwytrash_wheel_df) |> 
   relocate(trash_wheel_name)

 combine_df
```

The single tidy dataset named 'combine_df', contains 1033 observations, and has 'trash_wheel_name', 'dumpster', 'month', 'year','date' etc key variables.

Question:

1. What was the total weight of trash collected by Professor Trash Wheel?

```{r}
 trash_weight =
   proftrash_wheel_df |>
   summarise(total_weight_tons = sum(weight_tons, na.rm = TRUE))
 trash_weight 
```

The total weight of trash collected by Professor Trash Wheel is 247 tons.

2. What was the total number of cigarette butts collected by Gwynnda in June of 2022?

```{r}
 totol_cignumber = 
   gwytrash_wheel_df |> 
   filter(month == "June", year == "2022") |> 
   summarise(cigarette_butts = sum(cigarette_butts, na.rm = TRUE))
   
 totol_cignumber 
```

The total number of cigarette butts collected by Gwynnda in June of 2022 is 18120.

## Problem 3

Read and clean the "bakers.csv"

```{r}
 bakers_df =
   read_csv("gbb_datasets/bakers.csv", na = c("NA",".","")) |> 
   janitor::clean_names() |> 
   mutate(
     series = as.numeric(series),
     baker_age = as.character(baker_age),
  ) |> 
  separate(baker_name, into = 
             c("baker", "baker_last_name"), sep = " ", extra = "merge") 

```


Read and clean the "bakes.csv"

```{r}
 bakes_df =
   read_csv("gbb_datasets/bakes.csv", na = c("NA",".","")) |> 
   janitor::clean_names() |> 
  mutate(
    series = as.numeric(series),
    episode = as.character(episode)
  )
```


Read and clean the "results.csv"

```{r}
 results_df = 
   read_csv("gbb_datasets/results.csv", skip = 2, na = c("NA",".","")) |> 
   janitor::clean_names() |> 
   mutate(
     series = as.numeric(series),
     episode = as.character(episode)
  )
```


Check the 3 datasets using "anti_join" 

```{r}
 unmatched_bakers_in_bakes =
   anti_join(bakes_df, bakers_df, by = c("baker", "series"))

 unmatched_bakers_in_results =
   anti_join(results_df, bakers_df, by = c("baker", "series"))
```

Merge the 3 datasets into the final dataset, using "inner-join" so other variables will not be filtered out.

```{r}
 merged_df =
   bakes_df |> 
   inner_join(bakers_df, by = c("baker", "series")) |> 
   inner_join(results_df, by = c("baker", "series", "episode")) |> 
   select(baker, series, episode, baker_age, baker_occupation, hometown, signature_bake, show_stopper, result) |> 
   arrange(series, episode, baker, result)

  write.csv(merged_df, "gbb_datasets/cleaned_merged_dataset.csv")
```

Describe your data cleaning process, including any questions you have or choices you made. Briefly discuss the final dataset.

After I import "results.csv", I realize that the column names need to change, then I went back to the original "results.csv" when it first import. I found out the first 2 rows is not organized in a way R can read, so I skip the first 2 rows.

Questions:

1. Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10. Comment on this table – were there any predictable overall winners? Any surprises?

```{r}
 season_winner_df =
   results_df |>  
   filter(series >= 5, 
          result %in% c("Star Baker", "Winner")) |> 
   group_by(series, episode) |> 
   summarize(
    star_baker_or_winner = paste(baker, collapse=", "),  
    .groups = 'drop'  
  )

 season_winner_df
```



2. Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. 

```{r}
 viewers_df =
   read_csv("gbb_datasets/viewers.csv", na = c("NA",".","")) |> 
   janitor::clean_names() |> 
   pivot_longer(
     cols = series_1:series_10,
     names_to = "series",
     values_to = "viewership"
   ) |> 
 mutate(
    series = case_match(
      series, 
       "series_1" ~ 1,
       "series_2" ~ 2,
       "series_3" ~ 3,
       "series_4" ~ 4,
       "series_5" ~ 5,
       "series_6" ~ 6,
       "series_7" ~ 7,
       "series_8" ~ 8,
       "series_9" ~ 9,
       "series_10" ~ 10,
    ))

 head(viewers_df, 10)
   
```

Question: 
 What was the average viewership in Season 1? In Season 5?

```{r}
 average_season1 =
   viewers_df |> 
   filter(series == 1) |>      
   summarize(average_season1= mean(viewership, na.rm = TRUE)) 

 average_season1
 
  average_season5 =
   viewers_df |> 
   filter(series == 5) |>      
   summarize(average_season5= mean(viewership, na.rm = TRUE)) 
  
  average_season5
```

The average viewership in Season 1 is 2.77.
The average viewership in Season 5 is 10.0.
